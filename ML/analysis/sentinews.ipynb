{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitlambdaconda0fd89f066e574d87bb9fc614dcb859b8",
   "display_name": "Python 3.7.7 64-bit ('lambda': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk as nltk\n",
    "import sklearn\n",
    "from sklearn import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer,SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from nltk import tokenize\n",
    "import numpy as np\n",
    "import html\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\n",
    "    'display.max_colwidth', 0\n",
    ")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('hacker_news_sample.csv',nrows=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "        \n9  NaN                                                                       \n\n                                                                                            url  \\\n0  NaN                                                                                            \n1  NaN                                                                                            \n2  NaN                                                                                            \n3  NaN                                                                                            \n4  NaN                                                                                            \n5  NaN                                                                                            \n6  http://www.mcconnelling.org/                                                                   \n7  http://www.kickstarter.com/projects/whim/recycled-island                                       \n8  https://backchannel.com/when-facebook-cleared-out-thousands-of-rooms-ee42a4154b33#.hpgimx9i1   \n9  NaN                                                                                            \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  text  \\\n0  &gt;<i>which leads me to say why are you using C to do X?</i><p>Because they know C it&#x27;s fast and it has lots of libs available. They might also dislike Java or CL.<p>Not every engineering decision is perfect lots of factors play in.<p>&gt;<i>Attempts to combine the best of C (speed) with the best of scripting languages (easy to do things fast without having to pay attention to what you are doing) in my opinion end up merely joining the worst of both worlds rather than the best of both worlds.</i><p>The &quot;pay attention&quot; things is to needless complexity (memory management etc). They only reason we put up with those things was to get speed. If we can get adequate speed without those nobody cares about them.<p>&gt;<i>Besides isn&#x27;t programming about being specific? Do you really want to code stuff without having to worry about the details?</i><p>No programming is about getting results. Nobody cares about the details in the level of programming language minutuae.<p>We care about the &quot;effort put in&quot; and &quot;quality&#x2F;speed of results coming out&quot; ratio.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n1  I would like to point out some counter-examples:<p>«<i>Think of journalists. Many are losing their jobs. Newspapers are barely surviving. In the old days for every news event there were probably hundreds of journalists writing about the same story for their own local newspapers. Now because of the efficiency of the Internet and search engines a few journalists writing about it would suffice for the whole country. People would be able to find them. There is no reason why hundreds of newspapers should write and publish their own versions of the same story.</i>»<p>And yet you can still find incompetent people.  I've read so many inaccuracies grammar and typographic mistakes from information <i>professionals</i> (apparently) that makes me wonder if the price to pay for such efficiency is too high.  And I'm talking as a consumer information consumer.<p>«<i>Corporations are increasingly getting bigger (in terms of market caps) more global and more powerful yet they are getting smaller and smaller in terms of the number of people they employ because they have mastered the art of efficiency.</i>»<p>Okay take IKEA for example.  IKEA sells furniture and other home accessories world wide and everywhere you can find the same model.  However how many people do they employ on each store?  A hundred?  I don't know but I'm under the impression that is not a shrinking number.  And IKEA is a particularly good example because it's expansion model is not based on a franchise like fast food restaurants.<p>Of course I might not understood the intention of the author.   \n2  NaN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n3  <i>Our msbuild implementation can now build Project K and Roslyn</i><p>Wow. Really impressive -- our MSBuild hackery is gut-wrenching.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n4  No matter how awful iPhoto is it's still better than almost anything you can find on Linux. I know because that was one of my biggest gripes while I was using Ubuntu up to about 5 months ago.<p>And it's not about how Macs just work it's that you have apps for almost anything that actually work.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n5  The existence of a way to shard searches doesn't make scaling real time search on email (hint: do some back of the envelope calculation on how much data that involves) a non-issue.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n6  NaN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n7  NaN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n8  NaN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n9  The actual Internet of things is Tesla collecting 130 million miles of autopilot data to make autopilot safer. It&#x27;s GE collecting data from the jet engines they produce to understand failures and do predictive maintenance. It&#x27;s Netapp collecting error logs from fileservers in the field so they know how to prioritize their bug database which performance bottlenecks to fix and what limitations actual customers encounter.<p>That&#x27;s the real internet of things. Just ignore all the (hype for) idiotic connected home crap covered by the popular press. Yes vendors are producing these products but the only feature I want is a a global disable for any such thing I accidentally bring into my home.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n\n  dead             by  score          time     type        id      parent  \\\n0  NaN  coldtea       NaN     1.390844e+09  comment  7131680   7127578.0    \n1  NaN  etanol        NaN     1.319396e+09  comment  3146879   3145330.0    \n2  NaN  NaN           NaN     1.456641e+09  comment  11190089  11189361.0   \n3  NaN  Locke1689     NaN     1.407882e+09  comment  8170491   8170071.0    \n4  NaN  miloshadzic   NaN     1.362573e+09  comment  5330773   5327590.0    \n5  NaN  salsakran     NaN     1.302988e+09  comment  2454827   2452073.0    \n6  NaN  deepblueocean  2.0    1.395179e+09  story    7425232  NaN           \n7  NaN  tudorw         1.0    1.353326e+09  story    4803967  NaN           \n8  NaN  mirandak4      2.0    1.478268e+09  story    12872547 NaN           \n9  NaN  paulsutter    NaN     1.467500e+09  comment  12024085  12023632.0   \n\n   descendants  ranking deleted             timestamp  \n0 NaN          NaN       NaN     2014-01-27T17:31:13Z  \n1 NaN          NaN       NaN     2011-10-23T18:46:40Z  \n2 NaN          NaN       True    2016-02-28T06:26:56Z  \n3 NaN          NaN       NaN     2014-08-12T22:13:10Z  \n4 NaN          NaN       NaN     2013-03-06T12:28:02Z  \n5 NaN          NaN       NaN     2011-04-16T21:04:23Z  \n6  0.0         NaN       NaN     2014-03-18T21:44:46Z  \n7  0.0         NaN       NaN     2012-11-19T11:54:38Z  \n8  0.0         NaN       NaN     2016-11-04T13:55:30Z  \n9 NaN          NaN       NaN     2016-07-02T22:54:47Z  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>url</th>\n      <th>text</th>\n      <th>dead</th>\n      <th>by</th>\n      <th>score</th>\n      <th>time</th>\n      <th>type</th>\n      <th>id</th>\n      <th>parent</th>\n      <th>descendants</th>\n      <th>ranking</th>\n      <th>deleted</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>&amp;gt;&lt;i&gt;which leads me to say why are you using C to do X?&lt;/i&gt;&lt;p&gt;Because they know C it&amp;#x27;s fast and it has lots of libs available. They might also dislike Java or CL.&lt;p&gt;Not every engineering decision is perfect lots of factors play in.&lt;p&gt;&amp;gt;&lt;i&gt;Attempts to combine the best of C (speed) with the best of scripting languages (easy to do things fast without having to pay attention to what you are doing) in my opinion end up merely joining the worst of both worlds rather than the best of both worlds.&lt;/i&gt;&lt;p&gt;The &amp;quot;pay attention&amp;quot; things is to needless complexity (memory management etc). They only reason we put up with those things was to get speed. If we can get adequate speed without those nobody cares about them.&lt;p&gt;&amp;gt;&lt;i&gt;Besides isn&amp;#x27;t programming about being specific? Do you really want to code stuff without having to worry about the details?&lt;/i&gt;&lt;p&gt;No programming is about getting results. Nobody cares about the details in the level of programming language minutuae.&lt;p&gt;We care about the &amp;quot;effort put in&amp;quot; and &amp;quot;quality&amp;#x2F;speed of results coming out&amp;quot; ratio.</td>\n      <td>NaN</td>\n      <td>coldtea</td>\n      <td>NaN</td>\n      <td>1.390844e+09</td>\n      <td>comment</td>\n      <td>7131680</td>\n      <td>7127578.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2014-01-27T17:31:13Z</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>I would like to point out some counter-examples:&lt;p&gt;«&lt;i&gt;Think of journalists. Many are losing their jobs. Newspapers are barely surviving. In the old days for every news event there were probably hundreds of journalists writing about the same story for their own local newspapers. Now because of the efficiency of the Internet and search engines a few journalists writing about it would suffice for the whole country. People would be able to find them. There is no reason why hundreds of newspapers should write and publish their own versions of the same story.&lt;/i&gt;»&lt;p&gt;And yet you can still find incompetent people.  I've read so many inaccuracies grammar and typographic mistakes from information &lt;i&gt;professionals&lt;/i&gt; (apparently) that makes me wonder if the price to pay for such efficiency is too high.  And I'm talking as a consumer information consumer.&lt;p&gt;«&lt;i&gt;Corporations are increasingly getting bigger (in terms of market caps) more global and more powerful yet they are getting smaller and smaller in terms of the number of people they employ because they have mastered the art of efficiency.&lt;/i&gt;»&lt;p&gt;Okay take IKEA for example.  IKEA sells furniture and other home accessories world wide and everywhere you can find the same model.  However how many people do they employ on each store?  A hundred?  I don't know but I'm under the impression that is not a shrinking number.  And IKEA is a particularly good example because it's expansion model is not based on a franchise like fast food restaurants.&lt;p&gt;Of course I might not understood the intention of the author.</td>\n      <td>NaN</td>\n      <td>etanol</td>\n      <td>NaN</td>\n      <td>1.319396e+09</td>\n      <td>comment</td>\n      <td>3146879</td>\n      <td>3145330.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2011-10-23T18:46:40Z</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.456641e+09</td>\n      <td>comment</td>\n      <td>11190089</td>\n      <td>11189361.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>2016-02-28T06:26:56Z</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>&lt;i&gt;Our msbuild implementation can now build Project K and Roslyn&lt;/i&gt;&lt;p&gt;Wow. Really impressive -- our MSBuild hackery is gut-wrenching.</td>\n      <td>NaN</td>\n      <td>Locke1689</td>\n      <td>NaN</td>\n      <td>1.407882e+09</td>\n      <td>comment</td>\n      <td>8170491</td>\n      <td>8170071.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2014-08-12T22:13:10Z</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>No matter how awful iPhoto is it's still better than almost anything you can find on Linux. I know because that was one of my biggest gripes while I was using Ubuntu up to about 5 months ago.&lt;p&gt;And it's not about how Macs just work it's that you have apps for almost anything that actually work.</td>\n      <td>NaN</td>\n      <td>miloshadzic</td>\n      <td>NaN</td>\n      <td>1.362573e+09</td>\n      <td>comment</td>\n      <td>5330773</td>\n      <td>5327590.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2013-03-06T12:28:02Z</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>The existence of a way to shard searches doesn't make scaling real time search on email (hint: do some back of the envelope calculation on how much data that involves) a non-issue.</td>\n      <td>NaN</td>\n      <td>salsakran</td>\n      <td>NaN</td>\n      <td>1.302988e+09</td>\n      <td>comment</td>\n      <td>2454827</td>\n      <td>2452073.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2011-04-16T21:04:23Z</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>#McConnelling</td>\n      <td>http://www.mcconnelling.org/</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>deepblueocean</td>\n      <td>2.0</td>\n      <td>1.395179e+09</td>\n      <td>story</td>\n      <td>7425232</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2014-03-18T21:44:46Z</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>A floating self-sustaining home that would respond to rising sea levels</td>\n      <td>http://www.kickstarter.com/projects/whim/recycled-island</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>tudorw</td>\n      <td>1.0</td>\n      <td>1.353326e+09</td>\n      <td>story</td>\n      <td>4803967</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2012-11-19T11:54:38Z</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>What Ever Happened to Facebook's Rooms?</td>\n      <td>https://backchannel.com/when-facebook-cleared-out-thousands-of-rooms-ee42a4154b33#.hpgimx9i1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>mirandak4</td>\n      <td>2.0</td>\n      <td>1.478268e+09</td>\n      <td>story</td>\n      <td>12872547</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2016-11-04T13:55:30Z</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>The actual Internet of things is Tesla collecting 130 million miles of autopilot data to make autopilot safer. It&amp;#x27;s GE collecting data from the jet engines they produce to understand failures and do predictive maintenance. It&amp;#x27;s Netapp collecting error logs from fileservers in the field so they know how to prioritize their bug database which performance bottlenecks to fix and what limitations actual customers encounter.&lt;p&gt;That&amp;#x27;s the real internet of things. Just ignore all the (hype for) idiotic connected home crap covered by the popular press. Yes vendors are producing these products but the only feature I want is a a global disable for any such thing I accidentally bring into my home.</td>\n      <td>NaN</td>\n      <td>paulsutter</td>\n      <td>NaN</td>\n      <td>1.467500e+09</td>\n      <td>comment</td>\n      <td>12024085</td>\n      <td>12023632.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2016-07-02T22:54:47Z</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    df = df[df['text'].notna()]\n",
    "    df['neg'], df['neu'], df['pos'], df[\"compound\"] = [np.nan, np.nan,np.nan,np.nan]    \n",
    "    return df\n",
    "\n",
    "def add_sentiment(df):\n",
    "    df['neg'], df['neu'], df['pos'], df[\"compound\"] = [np.nan, np.nan,np.nan,np.nan]\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    for i, row in df.iterrows():\n",
    "        text = row[\"text\"]\n",
    "        text = html.unescape(text)\n",
    "        text =  re.sub('<[^<]+?>', '', text)\n",
    "        df.at[i,\"text\"] = text\n",
    "        ss = sid.polarity_scores(text)\n",
    "        for k in ss:\n",
    "            df.at[i,k] = ss[k]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /Users/ekaterinaromanovskaya/nltk_data...\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "             score          time            id        parent  descendants  \\\ncount  1737.000000  8.153000e+04  8.153000e+04  7.979300e+04  1548.000000   \nmean   5.213587     1.387459e+09  7.369586e+06  7.419128e+06  3.410853      \nstd    21.129398    7.622413e+07  4.220612e+06  4.213057e+06  14.535619     \nmin    0.000000     1.172086e+09  3.620000e+02  1.450000e+02 -1.000000      \n25%    1.000000     1.332827e+09  3.760196e+06  3.836577e+06 -1.000000      \n50%    1.000000     1.394251e+09  7.364090e+06  7.439650e+06  0.000000      \n75%    3.000000     1.454520e+09  1.102772e+07  1.106604e+07  2.000000      \nmax    639.000000   1.498466e+09  1.463506e+07  1.463497e+07  368.000000    \n\n       ranking           neg           neu           pos      compound  \ncount  0.0      81530.000000  81530.000000  81530.000000  81530.000000  \nmean  NaN       0.059936      0.825668      0.114299      0.216708      \nstd   NaN       0.082589      0.134709      0.119930      0.529816      \nmin   NaN       0.000000      0.000000      0.000000     -0.999500      \n25%   NaN       0.000000      0.761000      0.018000     -0.077200      \n50%   NaN       0.034000      0.839000      0.094000      0.273200      \n75%   NaN       0.092000      0.912000      0.161000      0.680800      \nmax   NaN       1.000000      1.000000      1.000000      0.999700      ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>score</th>\n      <th>time</th>\n      <th>id</th>\n      <th>parent</th>\n      <th>descendants</th>\n      <th>ranking</th>\n      <th>neg</th>\n      <th>neu</th>\n      <th>pos</th>\n      <th>compound</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1737.000000</td>\n      <td>8.153000e+04</td>\n      <td>8.153000e+04</td>\n      <td>7.979300e+04</td>\n      <td>1548.000000</td>\n      <td>0.0</td>\n      <td>81530.000000</td>\n      <td>81530.000000</td>\n      <td>81530.000000</td>\n      <td>81530.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>5.213587</td>\n      <td>1.387459e+09</td>\n      <td>7.369586e+06</td>\n      <td>7.419128e+06</td>\n      <td>3.410853</td>\n      <td>NaN</td>\n      <td>0.059936</td>\n      <td>0.825668</td>\n      <td>0.114299</td>\n      <td>0.216708</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>21.129398</td>\n      <td>7.622413e+07</td>\n      <td>4.220612e+06</td>\n      <td>4.213057e+06</td>\n      <td>14.535619</td>\n      <td>NaN</td>\n      <td>0.082589</td>\n      <td>0.134709</td>\n      <td>0.119930</td>\n      <td>0.529816</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>1.172086e+09</td>\n      <td>3.620000e+02</td>\n      <td>1.450000e+02</td>\n      <td>-1.000000</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.999500</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.000000</td>\n      <td>1.332827e+09</td>\n      <td>3.760196e+06</td>\n      <td>3.836577e+06</td>\n      <td>-1.000000</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>0.761000</td>\n      <td>0.018000</td>\n      <td>-0.077200</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.000000</td>\n      <td>1.394251e+09</td>\n      <td>7.364090e+06</td>\n      <td>7.439650e+06</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>0.034000</td>\n      <td>0.839000</td>\n      <td>0.094000</td>\n      <td>0.273200</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>3.000000</td>\n      <td>1.454520e+09</td>\n      <td>1.102772e+07</td>\n      <td>1.106604e+07</td>\n      <td>2.000000</td>\n      <td>NaN</td>\n      <td>0.092000</td>\n      <td>0.912000</td>\n      <td>0.161000</td>\n      <td>0.680800</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>639.000000</td>\n      <td>1.498466e+09</td>\n      <td>1.463506e+07</td>\n      <td>1.463497e+07</td>\n      <td>368.000000</td>\n      <td>NaN</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.999700</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "df = preprocess_df(df)\n",
    "add_sentiment(df)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored = X_train.loc[X_train['compound'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'sklearn.pipeline' has no attribute 'fit_transform'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-4f5792769571>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscored\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'sklearn.pipeline' has no attribute 'fit_transform'"
     ]
    }
   ],
   "source": [
    "X = pipeline.fit_transform(scored[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer(analyzer='word')),\n",
    "        ('tfidf', TfidfTransformer()),    \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=None,\n                      max_features=None, max_leaf_nodes=None,\n                      min_impurity_decrease=0.0, min_impurity_split=None,\n                      min_samples_leaf=1, min_samples_split=2,\n                      min_weight_fraction_leaf=0.0, presort='deprecated',\n                      random_state=42, splitter='best')"
     },
     "metadata": {},
     "execution_count": 122
    }
   ],
   "source": [
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg.fit(X, scored[\"compound\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_test = X_test.loc[X_test['compound'] != 0]\n",
    "X2 = pipeline.transform(scored_test[\"text\"])\n",
    "print(scored_test[\"compound\"].shape)\n",
    "print(X2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.6365505601380854"
     },
     "metadata": {},
     "execution_count": 133
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "predictions = tree_reg.predict(X2)\n",
    "\n",
    "tree_mse = mean_squared_error(scored_test[\"compound\"], predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "def build_model():\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=[X.shape[1]]),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "  model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae', 'mse'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_3 (Dense)              (None, 64)                4916992   \n_________________________________________________________________\ndense_4 (Dense)              (None, 64)                4160      \n_________________________________________________________________\ndense_5 (Dense)              (None, 1)                 65        \n=================================================================\nTotal params: 4,921,217\nTrainable params: 4,921,217\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>, <class 'NoneType'>\nTrain on 41576 samples, validate on 10395 samples\nEpoch 1/10\n41576/41576 [==============================] - 102s 2ms/sample - loss: 0.1676 - mae: 0.3143 - mse: 0.1676 - val_loss: 0.1464 - val_mae: 0.2798 - val_mse: 0.1464\nEpoch 2/10\n41576/41576 [==============================] - 110s 3ms/sample - loss: 0.1047 - mae: 0.2321 - mse: 0.1047 - val_loss: 0.1346 - val_mae: 0.2717 - val_mse: 0.1346\nEpoch 3/10\n41576/41576 [==============================] - 105s 3ms/sample - loss: 0.0793 - mae: 0.1950 - mse: 0.0793 - val_loss: 0.1419 - val_mae: 0.2650 - val_mse: 0.1419\nEpoch 4/10\n41576/41576 [==============================] - 111s 3ms/sample - loss: 0.0593 - mae: 0.1650 - mse: 0.0593 - val_loss: 0.1399 - val_mae: 0.2785 - val_mse: 0.1399\nEpoch 5/10\n41576/41576 [==============================] - 105s 3ms/sample - loss: 0.0442 - mae: 0.1402 - mse: 0.0442 - val_loss: 0.1430 - val_mae: 0.2678 - val_mse: 0.1430\nEpoch 6/10\n41576/41576 [==============================] - 105s 3ms/sample - loss: 0.0335 - mae: 0.1217 - mse: 0.0335 - val_loss: 0.1464 - val_mae: 0.2794 - val_mse: 0.1464\nEpoch 7/10\n41576/41576 [==============================] - 101s 2ms/sample - loss: 0.0259 - mae: 0.1077 - mse: 0.0259 - val_loss: 0.1449 - val_mae: 0.2712 - val_mse: 0.1449\nEpoch 8/10\n41576/41576 [==============================] - 106s 3ms/sample - loss: 0.0208 - mae: 0.0973 - mse: 0.0208 - val_loss: 0.1442 - val_mae: 0.2729 - val_mse: 0.1442\nEpoch 9/10\n41576/41576 [==============================] - 98s 2ms/sample - loss: 0.0172 - mae: 0.0891 - mse: 0.0172 - val_loss: 0.1455 - val_mae: 0.2706 - val_mse: 0.1455\nEpoch 10/10\n41576/41576 [==============================] - 97s 2ms/sample - loss: 0.0146 - mae: 0.0829 - mse: 0.0146 - val_loss: 0.1484 - val_mae: 0.2752 - val_mse: 0.1484\n"
    }
   ],
   "source": [
    "history = model.fit(\n",
    "  X, scored[\"compound\"],\n",
    "  epochs=EPOCHS, validation_split = 0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>, <class 'NoneType'>\n[[0.6620629]]\n"
    }
   ],
   "source": [
    "def predict_text(text, model = model):\n",
    "    xtemp = pipeline.transform([text])\n",
    "    predictions = model.predict(xtemp)\n",
    "    print(predictions)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "\n",
    "txt = \"Reality is that while China blocks Facebook, Google, etc and smartly props up their own clones, it’s “aghast” at the American protectionism and xenophobic behaviour. How dare the Americans block a Chinese app?! China is not a democracy. It’s not interested in fairness. China is playing the long game. Just like the wars of the past were fought with little toy armies of a few thousand knights and noblemen marching into each other’s countries until someone decided to conscript their whole nation into battle, the West is fighting allowing China to pilfer its technology, wreak the environment, and compete with state backed organisations. Wanna compete with Huawei? Good luck sending in your company noblemen, China is sending their whole nation behind it.\"\n",
    "\n",
    "\n",
    "preds = predict_text(\"Impressive! Personally, I am a few thousand places behind, but still in the top 0.2%. How? I asked and answered a few hundred questions early on, years ago, when SO was new and interesting... Now those answers are old and, like most answers on SO, out of date. Usually when I google something technical and get seemingly the exact right question asked on SO, the answers are no longer correct. Software versions change. And yet I still get a steady trickle of votes, forever increasing the gulf between outdated and no-longer-participating people like me and anyone starting on SO today.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>, <class 'NoneType'>\n"
    }
   ],
   "source": [
    "predictions = model.predict(X2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.39776707368686554"
     },
     "metadata": {},
     "execution_count": 161
    }
   ],
   "source": [
    "tree_mse = mean_squared_error(scored_test[\"compound\"], predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse"
   ]
  }
 ]
}